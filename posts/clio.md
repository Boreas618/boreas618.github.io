# Clio: A Hardware-Software Co-Designed Disaggregated Memory System

**Memory disaggregation**: two separate **network-attached pools**, one with compute nodes (CNs) and one with memory nodes (MNs).

**Two MemDisagg Approaches**: MNs with/without computation power.

* **With Computation power**: cost of host server and performance and scalability limitations caused by the way NICs interact with the host serverâ€™s virtual memory system.
* **Without**: performance, security, and management problems.

## Proposed Approach

"***A sweet spot***" in the middle by proposing a hardware-based MemDisagg solution that has the right amount of processing power at MNs.

> **Scalability**
>
> * Multiple application processes running on different CNs can allocate memory from the same CBoard.
> * Each process having its own remote virtual memory address space, which can span multiple CBoards.

**Key Research Question**: limited hardware resources for 100 Gbps, microsecond-level average and tail latency for TBs of memory and thousands of concurrent clients?

---

**Main Idea**: eliminate state from the MN hardware.

* The MN can treat each of its incoming requests in isolation even if requests that the client issues can sometimes be inter-dependent.
* The MN hardware does not store metadata or deals with it.

**Advantages of No State Design**: the hardware pipeline does not stall. Constantly high throughput. Hardware processing does not need to wait for any slower metadata operations and thus has **bounded tail latency**. 

> [!note]
>
> å¦‚æœå­˜å‚¨stateï¼Œåˆ™éšç€è¿è¡Œæ—¶é—´å’Œæ¥å…¥çš„CNçš„æ•°é‡å¢é•¿ï¼ŒMNçš„è¿è¡Œæˆæœ¬ä¹Ÿä¼šæ˜¾è‘—å¢é•¿ï¼Œä¸èƒ½å¾ˆå¥½scaleã€‚ä½†æ˜¯å³ä½¿ä¸éœ€è¦ç­‰å¾…å…ƒæ•°æ®æ“ä½œï¼ŒCN clientæ•°é‡çš„å¢é•¿ä¼šä¸ä¼šä½¿MNçš„å“åº”æ—¶å»¶unboundedï¼Ÿ

***But we cannot really eliminate state from MN hardware.*** Three reasons are explained in the paper, including synchronization problems, memory operations with metadata, per-process/client metadata. What we should do is to eliminate as much state as we can. For example, redesign the page table (accessing page table is a data operation and )

---

**Approach 1: separate the metadata/control plane and the data plane**. The former running as software on a low-power ARM-based SoC at MN and the latter in hardware at MN.

> * Metadata operations like memory allocation usually need more memory but are rarer (thus not as performance critical).
> * Data operations (i.e., all memory accesses) should be fast and are best handled purely in hardware.

> [!note]
>
> è¿™é‡Œåœ¨MNè¿è¡Œmetadata operationsçš„ideaå’ŒMiraä¸­function offloadæœ‰ç‚¹åƒï¼Œåœ¨è¿œç«¯MNçš„ä½åŠŸè€—ARM-based SoCè¿è¡Œçš„å‡½æ•°è¦æ»¡è¶³ï¼šè°ƒç”¨é¢‘æ¬¡ä½ã€å¤§é‡è®¿é—®è¿œç«¯å†…å­˜ã€‚ä¸Miraä¸åŒçš„æ˜¯ï¼ŒClioåˆ©ç”¨ç¡¬ä»¶åŠ é€Ÿæ•°æ®æ“ä½œï¼ŒMiraä¸»è¦é€šè¿‡ç¼“å­˜åŠ é€Ÿæ•°æ®æ“ä½œã€‚

**Approach 2: re-design the memory and networking data plane so that most state can be managed only at the CN side.** MN never initiates requests. Therefore, the transport-layer services at MNs can be simplified. New transport protocol manages request IDs, transport logic, retransmission buffer, congestion, and incast control all at CNs.

## Goals & Related Works

* **Hosting large amounts of memory with high utilization**: each MN should host hundreds GBs to a few TBs of memory.
* **Supporting a huge number of concurrent clients**: allow many (e.g., thousands of) client processes running on tens of CNs to access and share an MN.
* **Low-latency and high-throughput**: match the state-of-the-art network speed, i.e., **100 Gbps** throughput (for bigger requests) and **sub-2 ğœ‡ğ‘ ** median end-to-end latency (for smaller requests).

* **Low tail latency**: long tails like RDMAâ€™s 16.8ğ‘šğ‘  remote memory access can be detrimental.
* **Protected memory accesses**.
* **Low cost**: CapEx and OpEx costs.
* **Flexible**

### Server-Based Disaggregated Memory

**Problems with RDMA**: scalability and tail-latency.

> * A process ($P_M$ ) running at an MN allocate memory in its virtual memory address space.
> * $P_M$â€‹ register the allocated memory (called a memory region, or **MR**) with the RDMA NIC (RNIC).
> * The host OS and MMU set up and manage the page table that maps $P_M$ â€™s virtual addresses to physical memory addresses.
> * To avoid always accessing host memory for address mapping, RNICs cache page table entries (PTEs)
>
> PTEs and MRs are cached in the RNIC. RDMA has serious performance (scalability) issues with either large memory (PTEs) or many disjoint memory regions (MRs).

### Physical Disaggregated Memory

Treat the memory node as raw, physical memory, a model we call **PDM**. To prevent applications from accessing raw physical memory, add an indirection layer at CNs in hardware or software to map client process VAs or keys to MN PAs.

* CNs need multiple network round trips to access an MN for complex operations.
* Require the client side to manage disaggregated memory.
* Security.

## Clio Overview

<img src="https://p.ipic.vip/6gx0fp.png" alt="Screenshot 2024-02-26 at 9.37.02â€¯PM" style="zoom:50%;" />

A non-transparent interface where applications (running at CNs) allocate and access disaggregated memory via explicit API calls. By design, Clioâ€™s APIs can also be called by a runtime to support a transparent interface and allow the use of unmodified user applications.

> [!Note]
>
> æ‰€ä»¥è¿™é‡Œçœ‹åˆ°Clioå’ŒMiraçš„ä¼ æ‰¿æ€§ï¼ŒMiraçš„å·¥ä½œå¯ä»¥çœå»explicit API callsï¼Œé€šè¿‡MLIRå’ŒruntimeååŒè¿ä½œæ¥é€æ˜åœ°å¤„ç†è¿œç¨‹å†…å­˜æ“ä½œã€‚

*(Context matter)* Apart from the regular virtual memory address space, each process has a separate Remote virtual memory Address Space (RAS).

*(Context matter)* Each application process has a unique global PID across all CNs which is assigned by Clio when the application starts.

**Asynchronous APIs**: non-blocking

* A calling thread proceeds after calling an asynchronous API and later calls `rpoll` to get the result.
* Asynchronous APIs follow a release order.

Ensure consistency between metadata and data operations, by ensuring that potentially conflicting operations execute synchronously in the program order.

**Clio threads and processes can share data even when they are not on the same CN.** Clio does not enforce cache coherence automatically and lets applications choose their own coherence protocols.

> [!Note]
>
> è¿™é‡Œåº”è¯¥æŒ‡çš„æ˜¯ping pongé—®é¢˜å§ï¼Ÿè™½ç„¶è¯´åœ¨ä¼ ç»ŸSRAM-based cacheçš„ç¯å¢ƒä¸‹ï¼Œping pongé—®é¢˜çš„è®¨è®ºå’Œè§£å†³æ–¹æ¡ˆæ˜¯ä¸å°‘çš„ï¼Œä½†æ˜¯åœ¨disaggregatedçš„ç¡¬ä»¶ç¯å¢ƒä¸­æ˜¯ä¸æ˜¯æœ‰ç‰¹æ®Šçš„é—®é¢˜ï¼Ÿç¨åéœ€è¦æ–‡çŒ®è°ƒç ”ä¸€ä¸‹ã€‚

**Roles**:

* CNs are regular servers each equipped with a regular Ethernet NIC and connected to a top-of-rack (ToR) switch.
* MNs are our customized devices directly connected to a ToR switch.
* Applications run at CNs on top of the user-space library called CLib, which is in charge of request ordering, request retry, congestion, and incast control.

**Components of MNs**:

* An ASIC which runs the hardware logic for all data accesses (we call it the fast path and prototyped it with FPGA)
* An ARM processor which runs software for handling metadata and control operations (i.e., the slow path)
* An FPGA which hosts application computation offloading (i.e., the extend path).

An incoming request arrives at the ASIC and travels through standard Ethernet physical and MAC layers and a Match-and-Action-Table (MAT) that decides which of the three paths the request should go to based on the request type.

