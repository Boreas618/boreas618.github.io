import{_ as a,C as i,o as l,c,H as o,w as n,Q as s,k as e,a as r}from"./chunks/framework.5ae544db.js";const S=JSON.parse('{"title":"","description":"","frontmatter":{},"headers":[],"relativePath":"papers/kuber-scheduling.md","filePath":"papers/kuber-scheduling.md"}'),d={name:"papers/kuber-scheduling.md"},h=e("img",{src:"https://p.ipic.vip/nj7wf6.png",style:{zoom:"100%"}},null,-1),u=s('<h2 id="background-of-containers-administration" tabindex="-1">Background of Containers Administration <a class="header-anchor" href="#background-of-containers-administration" aria-label="Permalink to &quot;Background of Containers Administration&quot;">​</a></h2><ul><li><p><strong>Containers</strong></p><p>Operating system-level virtualization technology provides isolated environment called containers within a common operating system. It comprises a group of one or more processes that are isolated from the rest of the system.</p><blockquote><p>Linux kernel tools for containers: cgroups, namespaces, and chroot.</p></blockquote></li><li><p><strong>Container Orchestrators</strong></p><blockquote><p>(Orchestrators) provide tools that help us automate the deployment, management, scaling, interconnection, and availability of our container-based applications.</p></blockquote><p>Kubernetes is an orchestrator of containers. There are other container orchestrators such as <strong>Docker Swarm</strong> and <strong>Mesos</strong> but Kubernetes is the de facto standard.</p></li><li><p><strong>Container Managers</strong></p><p><strong>Difference between container managers (like Docker) and orchestrators (like Kubernetes)</strong>: Container managers focus on handling containers in a limited scope, while container orchestrators are designed for broader applications, often in large, distributed systems with multiple nodes.</p></li><li><p><strong>Container Runtimes</strong></p><p>The container runtimes are softwares responsible for running containers. Example: LXC, RunC, CRun, or Kata.</p><p>Containerd and CRI-O are <strong>high-level container runtimes</strong> (which encapsulate LXC, RunC, ...) that implement the <strong>Open Container Initiative (OCI)</strong>, a standard specification for image formats and runtimes re- quirements providing container portability.</p><p>In order to be managed by Kubernets, Containerd and CRI-O both implement the <strong>standard Container Runtime Interface (CRI)</strong>.</p></li></ul><p>The above concepts can be reflected in the following figure:</p>',3),p=e("img",{src:"https://p.ipic.vip/79ll8w.png",alt:"Screenshot 2023-11-15 at 12.59.47 AM",style:{zoom:"50%"}},null,-1),g=e("p",null,[e("code",null,"kubelet"),r(" is the daemon component of the Kubernetes orchestrator that communicates with the high-level container runtime.")],-1),f=e("p",null,[e("code",null,"Docker Daemon"),r(" acts as a container manager with an API that simplifies the management of the lifecycle of the containers and communicates with containerd.")],-1),m=e("ul",null,[e("li",null,[e("p",null,[e("strong",null,"Pod"),r(": A pod is the basic deployment unit that can be operated and managed in the cluster.")])]),e("li",null,[e("p",null,[e("strong",null,"Node"),r(": A node can be a physical or virtual machine.")])])],-1),b=e("img",{src:"https://miro.medium.com/v2/resize:fit:1344/1*vJp5o7ABILiIapesES8j6g.png",alt:"img",style:{zoom:"50%"}},null,-1),k=s('<blockquote><p>Using software-defined overlay networks, such as Flannel or Calico, allows K8s to assign a unique IP address to each pod and service.</p></blockquote><p><strong>Component of Mater Nodes</strong>: etcd, scheduler, API server and controllers.</p><h2 id="scheduling-user-specifications" tabindex="-1">Scheduling: User Specifications <a class="header-anchor" href="#scheduling-user-specifications" aria-label="Permalink to &quot;Scheduling: User Specifications&quot;">​</a></h2><p>Workloads of a given set of nodes: <strong>affinity</strong>, <strong>taint</strong> and <strong>toleration</strong>.</p><ul><li><p><strong>Affinity</strong> is <strong>a property of pods</strong> that attracts them to a set of nodes either as a preference or a hard requirement).</p></li><li><p><strong>Taint</strong> is <strong>a property of nodes</strong> that repel a set of pods.</p></li><li><p><strong>Toleration</strong> is <strong>a property of pods</strong> that allows the scheduler to schedule pods with matching taints.</p></li></ul><p>The <a href="https://kubernetes.io/docs/reference/command-line-tools-reference/kube-scheduler/" target="_blank" rel="noreferrer">kube-scheduler</a> uses <strong>request</strong> to decide which node to place the Pod on. The <a href="https://kubernetes.io/docs/reference/generated/kubelet" target="_blank" rel="noreferrer">kubelet</a> enforces those <strong>limit</strong>s so that the running container is not allowed to use more of that limit. The kubelet also reserves at least the <strong>request</strong> amount of that resource specifically for that container to use. If the node where a pod is running has enough of a resource available, it&#39;s possible (and allowed) for a container to use more resource than its <code>request</code> for that resource specifies. However, a container is not allowed to use more than its resource <code>limit</code>.</p><p>Nodes dealing with lack of resources: based on QoS class.</p><p><strong>Best-effort</strong>: the pod can use as much resource above the request quota as posssible and will be evicted if the node is under resource pressure.</p><p><strong>Guranteed</strong>: the pods specify a request value equal to the limit. They are guaranteed not to be killed until they exceed their limits or there are no lower-priority Pods that can be preempted from the Node.</p><p><strong>Burstable</strong>: have lower-bound resource guarantees based on the request, but do not require a specific limit. If a limit is not specified, it defaults to a limit equivalent to the capacity of the Node, which allows the Pods to use as much as resource as available. These Pods are evicted after all <code>BestEffort</code> Pods are evicted.</p><h2 id="scheduling-internal-workflow" tabindex="-1">Scheduling: Internal Workflow <a class="header-anchor" href="#scheduling-internal-workflow" aria-label="Permalink to &quot;Scheduling: Internal Workflow&quot;">​</a></h2><ol><li>The request is authenticated first and validated.</li><li>The <strong>API server</strong> on the <strong>mater node</strong> creates a <strong>pod object</strong>, without assigning it to a node, updates the information of the newly created pod in <strong>etcd</strong> and updated/shows us a message that a <strong>pod</strong> is got created.</li><li>The <strong>scheduler</strong> which is continually monitoring the <strong>API server</strong> gets to know that a new pod is got created with no node assigned to it.</li><li>The <strong>scheduler</strong> identifies the right node to place the new <strong>pod</strong> and communicate back to the <strong>API server</strong> (with the information of the right node for the pod)</li><li>The <strong>API server</strong> again updates the information to the <strong>etcd</strong> received from <strong>Scheduler</strong>.</li><li>The <strong>API server</strong> then passed the same information to the <strong>kubelet agent</strong> on the appropriate <strong>worker node</strong> identified by <strong>scheduler</strong> in the 4th step.</li><li>The <strong>kubelet</strong> then creates the pod on node and instructs the <strong>container runtime engine</strong> to deploy the application image/container.</li><li>Once done, the <strong>kubelet</strong> updates the information/status of the pod back to the <strong>API server</strong>.</li><li>And <strong>API server</strong> updates the information/data back in the <strong>etcd</strong>. [2]</li></ol><p>At runtime, users can modify the resource configuration by submitting an update request of the YAML description file to the master node.</p><blockquote><p>Note that the K8s API server uses optimistic concurrency (when the API server detects concurrent write attempts, it rejects the latter of the two write operations.)</p></blockquote>',14),_=e("img",{src:"https://phoenixnap.com/kb/wp-content/uploads/2021/04/full-kubernetes-model-architecture.png",alt:"Understanding Kubernetes Architecture with Diagrams",style:{zoom:"50%"}},null,-1),w=s('<h3 id="node-selection-in-kube-scheduler-1" tabindex="-1">Node Selection in kube-scheduler [1] <a class="header-anchor" href="#node-selection-in-kube-scheduler-1" aria-label="Permalink to &quot;Node Selection in kube-scheduler [1]&quot;">​</a></h3><p>kube-scheduler is the default scheduler for Kubernetes and runs as part of the control plane.</p><p>kube-scheduler selects a node for the pod in a 2-step operation:</p><ol><li><strong>Filtering</strong> For example, the PodFitsResources filter checks whether a candidate Node has enough available resource to meet a Pod&#39;s specific resource requests.</li><li><strong>Scoring</strong> The scheduler assigns a score to each Node that survived filtering, basing this score on the active scoring rules. The highest percentage of free CPU and memory is a commonly used ranking criterion (<strong>LeastRequestPriority</strong> policy)</li></ol><h2 id="references" tabindex="-1">References <a class="header-anchor" href="#references" aria-label="Permalink to &quot;References&quot;">​</a></h2><p><strong>1</strong> Kubernetes Scheduler <a href="https://kubernetes.io/docs/concepts/scheduling-eviction/kube-scheduler/" target="_blank" rel="noreferrer">https://kubernetes.io/docs/concepts/scheduling-eviction/kube-scheduler/</a></p><p><strong>2</strong> Kubernetes Workflow for Absolute Beginners <a href="https://technos.medium.com/kubernetes-workflow-bad346c54962" target="_blank" rel="noreferrer">https://technos.medium.com/kubernetes-workflow-bad346c54962</a></p>',7);function v(y,T,q,P,A,C){const t=i("center");return l(),c("div",null,[o(t,null,{default:n(()=>[h]),_:1}),u,o(t,null,{default:n(()=>[p]),_:1}),g,f,m,o(t,null,{default:n(()=>[b]),_:1}),k,o(t,null,{default:n(()=>[_]),_:1}),w])}const N=a(d,[["render",v]]);export{S as __pageData,N as default};
